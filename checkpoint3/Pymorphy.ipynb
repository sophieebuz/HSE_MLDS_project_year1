{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Комментарий_ : данный ноутбук не запущен, поскольку многие вычисления выполнялись в разных ноутбуках и не единовременно. Таким образом, данный ноутбку является обобщающим и поясняющим весь ход работы. Полностью обработанный датасет можно найти на гитхабе в файле READ.ME.  \n",
    "Также в данном ноутбуке полностью опущена работа с датасетом (соединение отедльных файлов, добавление колонки с обработанными данными, склеивание датасетов и тд.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Очиста данных и обработка текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удаляем новости, датируемые 1914 годом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_csv(\"lenta-ru-news.csv\")\n",
    "df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# переводим дату в формат даты\n",
    "df_full['date'] = pd.to_datetime(df_full['date'], format = '%Y-%m-%d')\n",
    "\n",
    "# удаляет 2014 год\n",
    "df = df_full[df_full['date'].dt.year != 1914]\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импорт нужных библиотек и моделей для обработки текста новостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import Doc\n",
    "from natasha import Segmenter\n",
    "segmenter = Segmenter()\n",
    "\n",
    "import string\n",
    "print(string.punctuation)\n",
    "spec_chars = string.punctuation + '\\n«»\\t—…–№' \n",
    "spec_chars\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "unique_stops = set(stopwords.words('russian'))\n",
    "\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку наши данные представлены в виде связного текста, нам необходимо преобразовать их в понятный для машины язык - набор векторов. Перед этим необходимо провести обработку всех текстов и преобразовать их в массивы слов в начальной форме.\n",
    "\n",
    "**Общий подход**  \n",
    ">Подготовка данных для последующей кодировки была произведена двумя путями. Основное отличие подходов заключалось в разных принципах работы лемматизаторов (библиотеки pymystem3 и pymorphy2).\n",
    ">\n",
    ">Выбранные библиотеки были разработаны для обработки текстов русского языка. На данном этапе отпала всемиизвестная библиотека nltk. По визуальному анализу токенизированных текстов было видно, что она работает на порядок хуже.\n",
    ">\n",
    ">Отличие между этими библиотеками состоит в том, что pymystem3 токенизирует и лемматизирует слова с учетом контекста слова, что является большим преимуществом для русского языка. А библитека pymorphy2 лемматизиует слова уже после токенизации. Ее приятная черта в том, что можно настроить лемматизацию вручную.\n",
    "\n",
    "\n",
    "____\n",
    "Данный ноутбук содержит обработку текстов с помощью библиотеки - **pymorphy2**. Как уже было сказано ранее, в отличие от Mystem данный лемматизатор определяет часть речи и приводит к начальной форме без учета контекста слова. Это имеет свои недостатки (например фраза `сделан из стали` будет выглядеть как `\"сделать\", \"из\", \"стать\"`, а нет `\"сталь\"`, как следует из контекста).  \n",
    "___\n",
    "Также эта библиотека работает уже с токенизированными словами. Таком образом, предварительно требуется токенизация текста.\n",
    "\n",
    "Здесь был выбор из двух библиотек: классическая nltk и русская natasha. Проводилось тестирование работы обоих библиотек на небольшой части датасета и применением после визуального анализа результатов. Практически сразу было видно явное преимущество по качеству библиотеки **natasha**, разработанную в России специально для обработки текстов русского языка. \n",
    ">Примером может послужить следующая фраза: `«битвы…Величие»`. Фраза взята из одной новости. К сожалению, подобные опечатки довольно часто встречались в датасете. Библиотека nltk не распознавала из и после токенизации они так и оставались «слипшимися». Однако Natasha спокойно с этим справлялась и выдавала `«битвы» и «Величие»`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция, которая обрабатывает текст. На вход принимает текст одной новости.\n",
    "\n",
    "Обработка текста была выстроена по следующей логике:\n",
    "- импорты библиотек, потому что при обработке было применено распараллеливание и функции импортированные глобально не видны\n",
    "- если новость была пустой (nan), обработка не производилась и возвращалось значение \"\\$\\_\\$nan\\$\\_\\$\"\n",
    "- токенизация\n",
    "- удаление лишних символов (по типу !\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~/n«»\\t—…–№)\n",
    "- удаление стоп-слов (словарь стоп-слов был взят из библиотеки nltk)\n",
    "- лемматизация с помощью библиотеки pymorphy\n",
    "\n",
    "Отдельное приведение в нижний регистр слов отсутствовало, поскольку Natasha при токенизации автоматически переводит слова в нижний регистр."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_processing(text):\n",
    "    from natasha import Doc\n",
    "    from natasha import Segmenter\n",
    "    segmenter = Segmenter()\n",
    "\n",
    "    import string\n",
    "    #print(string.punctuation)\n",
    "    spec_chars = string.punctuation + '\\n«»\\t—…–№' \n",
    "    spec_chars\n",
    "\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    unique_stops = set(stopwords.words('russian'))\n",
    "\n",
    "    import pymorphy2\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    \n",
    "    if str(text) == 'nan':\n",
    "        return '$_$nan$_$'\n",
    "    \n",
    "    else:\n",
    "        # переводим текст в формат наташи\n",
    "        doc = Doc(text)\n",
    "\n",
    "        # разбиение на токены\n",
    "        doc.segment(segmenter)\n",
    "        #doc.tokens\n",
    "\n",
    "        # Очистка списка от лишних знаков (подумать над двойными кавычками: '', \"\", ``) --upd убрала тупым образом\n",
    "        doc_natasha = []\n",
    "        for i in range(len(doc.tokens)):\n",
    "            if doc.tokens[i].text not in spec_chars:\n",
    "                doc_natasha.append(doc.tokens[i].text)\n",
    "\n",
    "\n",
    "        # удаление стоп-слов и перевод в нижний регистр\n",
    "        no_stops = []\n",
    "        for token in doc_natasha:\n",
    "            token1 = token.lower()\n",
    "            if token1 not in unique_stops: #and token.isalpha():\n",
    "                no_stops.append(token1)\n",
    "\n",
    "        # сложная лемматизация\n",
    "        doc_lem = []\n",
    "        for i in range(len(no_stops)):\n",
    "            word1 = morph.parse(no_stops[i])[0]\n",
    "            if ('INFN' in word1.tag) or ('VERB' in word1.tag) or ('GRND' in word1.tag) or ('PRTF' in word1.tag) or ('PRTS' in word1.tag):\n",
    "                word = word1.normalized.word\n",
    "                doc_lem.append(word)\n",
    "            else:\n",
    "                try:\n",
    "                    word = word1.inflect({'nomn'}).word\n",
    "                    doc_lem.append(word)\n",
    "                except:\n",
    "                    word = word1.normal_form\n",
    "                    doc_lem.append(word)\n",
    "\n",
    "        return doc_lem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импорт библиотеки для распараллеливания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocess import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее расчеты производились следующим образом:  \n",
    "Поскольку файл очень объемный на каждой итерации цикла считывалось определенное кол-во строк из датасета, которые затем посылались в вышеописанную функцию (+ применялось распараллеливания для ускорения работы). Затем после обработки этого пула строк они в виде датафрейма сохранялись в csv, чтобы избежать потерь на случай прерывании работы функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short = pd.DataFrame(columns=['text'])\n",
    "df_short.to_csv('file_for_save.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "for i in tqdm(range(5)):\n",
    "    # Считывание текста по кускам\n",
    "    my_df = pd.read_csv(\"lenta-ru-news.csv\", nrows=100, header=0, skiprows=range(1, 10000*2+100*i*2+1))\n",
    "    news_text = my_df['text']\n",
    "    test=[]\n",
    "   \n",
    "\n",
    "    with Pool(processes=4) as pool:\n",
    "        for i in tqdm(pool.imap(word_processing, iter(news_text))):\n",
    "            test.append(i)\n",
    "            df_short.at[len(test)-1, 'text'] = i\n",
    "\n",
    "    df_short.to_csv('file_for_save.csv', index=False, mode='a', header=False)\n",
    "    df_short.drop(df_short.index[:], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция обрабатывала 10тыс текстов за 8.30 минут."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изначально функция имела другой вид. Это было связано с тем, что в датасете имелись плохо написанные слова (например \"пустыниСражались\"), то есть они были слипшиеся. Для исправления этой проблемы было решено использовать **YandexSpeller**.\n",
    "\n",
    "Однако потом пришлось отказаться от его применения по нескольким причинам. Во-первых, это было очень это было очень времязатратно, потому что спеллер мог исправлять ошибки только на небольшом фрагменте текста и приходилось ему давать по одному слову из списка из токенизированных слов. Во-вторых, не ней стоит ограничение по кол-посылок в день (оно довольно небольшое). \n",
    "\n",
    "Функция работала слишком долго (весь датасет обработался примерно бы за 8 суток), поэтому от исправления ошибок пришлось отказаться и пожертвовать \"слипшимися\" словами."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
